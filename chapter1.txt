hajakkl;ok
Use the following add-permission command to trust the CloudWatch Events service principal (events.amazonaws.com) and scope permissions to the rule with the specified Amazon Resource Name (ARN):

aws lambda add-permission \
--function-name LogScheduledEvent \
--statement-id my-scheduled-event \
--action 'lambda:InvokeFunction' \
--principal events.amazonaws.com \
--source-arn arn:aws:events:us-east-1:123456789012:rule/my-scheduled-rule

Use the following put-targets command to add the Lambda function that you created to this rule so that it runs every five minutes:

aws events put-targets --rule my-scheduled-rule --targets file://targets.json

Create the file targets.json with the following contents:

    [
      {
        "Id": "1", 
        "Arn": "arn:aws:lambda:us-east-1:123456789012:function:LogScheduledEvent"
      }
    ]

Step 3: Verify the Rule

At least five minutes after completing Step 2, you can verify that your Lambda function was invoked.

To test your rule

    Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/

.

Inggg the navigation pane, choose Events, Rules, select the name of the rule that you created, and choose Show metrics for the rule.

To view the output from your Lambda function, do the following:

    In the navigation pane, choose Logs.

    Select the name of the log group for your Lambda function (/aws/lambda/function-name).

    Select the name of log stream to view the data provided by t
    usiness continuity is important for building mission-critical workloads on AWS. As an AWS customer, you might define recovery point objectives (RPO) and recovery time objectives (RTO) for different tier applications in your business. After the RPO and RTO requirements are defined, it is up to your architects to determine how to meet those requirements.

You probably store persistent data in Amazon EBS volumes, which live within a single Availability Zone. And, following best practices, you take snapshots of your EBS volumes to back up the data on Amazon S3, which provides 11 9’s of durability. If you are following these best practices, then you’ve probably recognized the need to manage the number of snapshots you keep for a particular EBS volume and delete older, unneeded snapshots. Doing this cleanup helps save on storage costs.

Some customers also have policies stating that backups need to be stored a certain number of miles away as part of a disaster recovery (DR) plan. To meet these requirements, customers copy their EBS snapshots to the DR region. Then, the same snapshot management and cleanup has to also be done in the DR region.

All of this snapshot management logic consists of different components. You would first tag your snapshots so you could manage them. Then, determine how many snapshots you currently have for a particular EBS volume and assess that value against a retention rule. If the number of snapshots was greater than your retention value, then you would clean up old snapshots. And finally, you might copy the latest snapshot to your DR region. All these steps are just an example of a simple snapshot management workflow. But how do you automate something like this in AWS? How do you do it without servers?

One of the most powerful AWS services released in 2016 was Amazon Cloud
hajakkl
jhjhhj